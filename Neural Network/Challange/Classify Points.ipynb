{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Logistic_X_train.csv',header='infer')\n",
    "Y_train = pd.read_csv('Logistic_Y_train.csv',header='infer')\n",
    "X_test = pd.read_csv('Logistic_X_test.csv',header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 2) (2250,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.values\n",
    "Y_train = Y_train.values.reshape((-1,))\n",
    "X_test = X_test.values\n",
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    e_pa = np.exp(a)\n",
    "    ans = e_pa/np.sum(e_pa,axis=1,keepdims=True)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,layers,output_size):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        \n",
    "        model= {}\n",
    "        \n",
    "        model['W1'] = np.random.randn(input_size,layers[0])\n",
    "        model['b1'] = np.zeros((1,layers[0]))\n",
    "        \n",
    "        model['W2'] = np.random.randn(layers[0],layers[1])\n",
    "        model['b2'] = np.zeros((1,layers[1]))\n",
    "        \n",
    "        model['W3'] = np.random.randn(layers[1],output_size)\n",
    "        model['b3'] = np.zeros((1,output_size))\n",
    "        \n",
    "        self.model = model\n",
    "        self.activation_output = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        b1,b2,b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        \n",
    "        z1 = np.dot(x,W1)+b1\n",
    "        a1 = np.tanh(z1)\n",
    "        \n",
    "        z2 = np.dot(a1,W2)+b2\n",
    "        a2 = np.tanh(z2)\n",
    "        \n",
    "        z3 = np.dot(a2,W3)+b3\n",
    "        y_ = softmax(z3)\n",
    "        \n",
    "        self.activation_output = (a1,a2,y_)\n",
    "        return y_\n",
    "    def backward(self,x,y,learning_rate):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        b1,b2,b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        a1,a2,y_ = self.activation_output\n",
    "        \n",
    "        delta3 = y_-y\n",
    "        dw3 = np.dot(a2.T,delta3)\n",
    "        db3 = np.sum(delta3,axis=0)\n",
    "        \n",
    "        delta2 = (1-np.square(a2))*np.dot(delta3,W3.T)\n",
    "        dw2 = np.dot(a1.T,delta2)\n",
    "        db2 = np.sum(delta2,axis=0)\n",
    "        \n",
    "        delta1 = (1-np.square(a1))*np.dot(delta2,W2.T)\n",
    "        dw1 = np.dot(x.T,delta1)\n",
    "        db1 = np.sum(delta1,axis=0)\n",
    "        \n",
    "        self.model['W1'] -= learning_rate*dw1\n",
    "        self.model['b1'] -= learning_rate*db1\n",
    "        \n",
    "        self.model['W2'] -= learning_rate*dw2\n",
    "        self.model['b2'] -= learning_rate*db2\n",
    "        \n",
    "        self.model['W3'] -= learning_rate*dw3\n",
    "        self.model['b3'] -= learning_rate*db3\n",
    "        \n",
    "    def predict(self,x):\n",
    "        y_ = self.forward(x)\n",
    "        return np.argmax(y_,axis=1)\n",
    "    \n",
    "    def summary(self):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        a1,a2,y_ = self.activation_outputs\n",
    "        \n",
    "        print(\"W1\",W1.shape)\n",
    "        print(\"A1\",a1.shape)\n",
    "        \n",
    "        print(\"W2\",W2.shape)\n",
    "        print(\"A2\",a2.shape)\n",
    "        \n",
    "        print(\"W3\",W3.shape)\n",
    "        print(\"Y_\",y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hot,p):\n",
    "    return -np.mean(y_hot*np.log(p))\n",
    "\n",
    "def one_hot(y,depth):\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    y_hot = np.zeros((m,depth))\n",
    "    y_hot[np.arange(m),y] = 1\n",
    "    \n",
    "    return y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=X_train.shape[1],layers=[15,5],output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,y,model,learning_rate=0.001,maxItr=500,logs=True):\n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    classes=2\n",
    "    y_hot = one_hot(y,classes)\n",
    "    \n",
    "    for i in range(maxItr):\n",
    "        \n",
    "        Y_ = model.forward(x)\n",
    "        l = loss(y_hot,Y_)\n",
    "        training_loss.append(l)\n",
    "        model.backward(x,y_hot,learning_rate)\n",
    "        \n",
    "        if(logs and i%20==0):\n",
    "            print(\"Epoch %d Loss %.4f\"%(i,l))\n",
    "        \n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.1966\n",
      "Epoch 20 Loss 0.2206\n",
      "Epoch 40 Loss 0.1363\n",
      "Epoch 60 Loss 0.0915\n",
      "Epoch 80 Loss 0.0549\n",
      "Epoch 100 Loss 0.0509\n",
      "Epoch 120 Loss 0.0487\n",
      "Epoch 140 Loss 0.0762\n",
      "Epoch 160 Loss 0.0475\n",
      "Epoch 180 Loss 0.0483\n",
      "Epoch 200 Loss 0.0471\n",
      "Epoch 220 Loss 0.0490\n",
      "Epoch 240 Loss 0.0496\n",
      "Epoch 260 Loss 0.0564\n",
      "Epoch 280 Loss 0.0699\n",
      "Epoch 300 Loss 0.0475\n",
      "Epoch 320 Loss 0.0518\n",
      "Epoch 340 Loss 0.0518\n",
      "Epoch 360 Loss 0.0508\n",
      "Epoch 380 Loss 0.0487\n",
      "Epoch 400 Loss 0.0495\n",
      "Epoch 420 Loss 0.0487\n",
      "Epoch 440 Loss 0.0485\n",
      "Epoch 460 Loss 0.0483\n",
      "Epoch 480 Loss 0.0481\n",
      "Epoch 500 Loss 0.0479\n",
      "Epoch 520 Loss 0.0477\n",
      "Epoch 540 Loss 0.0476\n",
      "Epoch 560 Loss 0.0475\n",
      "Epoch 580 Loss 0.0474\n",
      "Epoch 600 Loss 0.0473\n",
      "Epoch 620 Loss 0.0472\n",
      "Epoch 640 Loss 0.0471\n",
      "Epoch 660 Loss 0.0471\n",
      "Epoch 680 Loss 0.0470\n",
      "Epoch 700 Loss 0.0469\n",
      "Epoch 720 Loss 0.0469\n",
      "Epoch 740 Loss 0.0468\n",
      "Epoch 760 Loss 0.0468\n",
      "Epoch 780 Loss 0.0467\n",
      "Epoch 800 Loss 0.0467\n",
      "Epoch 820 Loss 0.0466\n",
      "Epoch 840 Loss 0.0466\n",
      "Epoch 860 Loss 0.0465\n",
      "Epoch 880 Loss 0.0465\n",
      "Epoch 900 Loss 0.0464\n",
      "Epoch 920 Loss 0.0464\n",
      "Epoch 940 Loss 0.0463\n",
      "Epoch 960 Loss 0.0463\n",
      "Epoch 980 Loss 0.0462\n",
      "Epoch 1000 Loss 0.0462\n",
      "Epoch 1020 Loss 0.0462\n",
      "Epoch 1040 Loss 0.0461\n",
      "Epoch 1060 Loss 0.0461\n",
      "Epoch 1080 Loss 0.0461\n",
      "Epoch 1100 Loss 0.0461\n",
      "Epoch 1120 Loss 0.0460\n",
      "Epoch 1140 Loss 0.0460\n",
      "Epoch 1160 Loss 0.0460\n",
      "Epoch 1180 Loss 0.0460\n",
      "Epoch 1200 Loss 0.0460\n",
      "Epoch 1220 Loss 0.0460\n",
      "Epoch 1240 Loss 0.0460\n",
      "Epoch 1260 Loss 0.0460\n",
      "Epoch 1280 Loss 0.0459\n",
      "Epoch 1300 Loss 0.0459\n",
      "Epoch 1320 Loss 0.0459\n",
      "Epoch 1340 Loss 0.0459\n",
      "Epoch 1360 Loss 0.0459\n",
      "Epoch 1380 Loss 0.0459\n",
      "Epoch 1400 Loss 0.0459\n",
      "Epoch 1420 Loss 0.0459\n",
      "Epoch 1440 Loss 0.0458\n",
      "Epoch 1460 Loss 0.0458\n",
      "Epoch 1480 Loss 0.0458\n",
      "Epoch 1500 Loss 0.0458\n",
      "Epoch 1520 Loss 0.0458\n",
      "Epoch 1540 Loss 0.0457\n",
      "Epoch 1560 Loss 0.0457\n",
      "Epoch 1580 Loss 0.0457\n",
      "Epoch 1600 Loss 0.0456\n",
      "Epoch 1620 Loss 0.0456\n",
      "Epoch 1640 Loss 0.0456\n",
      "Epoch 1660 Loss 0.0455\n",
      "Epoch 1680 Loss 0.0455\n",
      "Epoch 1700 Loss 0.0455\n",
      "Epoch 1720 Loss 0.0454\n",
      "Epoch 1740 Loss 0.0454\n",
      "Epoch 1760 Loss 0.0453\n",
      "Epoch 1780 Loss 0.0453\n",
      "Epoch 1800 Loss 0.0453\n",
      "Epoch 1820 Loss 0.0452\n",
      "Epoch 1840 Loss 0.0452\n",
      "Epoch 1860 Loss 0.0451\n",
      "Epoch 1880 Loss 0.0451\n",
      "Epoch 1900 Loss 0.0450\n",
      "Epoch 1920 Loss 0.0450\n",
      "Epoch 1940 Loss 0.0449\n",
      "Epoch 1960 Loss 0.0449\n",
      "Epoch 1980 Loss 0.0448\n",
      "Epoch 2000 Loss 0.0448\n",
      "Epoch 2020 Loss 0.0447\n",
      "Epoch 2040 Loss 0.0446\n",
      "Epoch 2060 Loss 0.0446\n",
      "Epoch 2080 Loss 0.0445\n",
      "Epoch 2100 Loss 0.0445\n",
      "Epoch 2120 Loss 0.0444\n",
      "Epoch 2140 Loss 0.0443\n",
      "Epoch 2160 Loss 0.0443\n",
      "Epoch 2180 Loss 0.0442\n",
      "Epoch 2200 Loss 0.0441\n",
      "Epoch 2220 Loss 0.0440\n",
      "Epoch 2240 Loss 0.0440\n",
      "Epoch 2260 Loss 0.0439\n",
      "Epoch 2280 Loss 0.0437\n",
      "Epoch 2300 Loss 0.0436\n",
      "Epoch 2320 Loss 0.0435\n",
      "Epoch 2340 Loss 0.0434\n",
      "Epoch 2360 Loss 0.0432\n",
      "Epoch 2380 Loss 0.0431\n",
      "Epoch 2400 Loss 0.0430\n",
      "Epoch 2420 Loss 0.0429\n",
      "Epoch 2440 Loss 0.0428\n",
      "Epoch 2460 Loss 0.0427\n",
      "Epoch 2480 Loss 0.0427\n",
      "Epoch 2500 Loss 0.0426\n",
      "Epoch 2520 Loss 0.0426\n",
      "Epoch 2540 Loss 0.0425\n",
      "Epoch 2560 Loss 0.0425\n",
      "Epoch 2580 Loss 0.0424\n",
      "Epoch 2600 Loss 0.0424\n",
      "Epoch 2620 Loss 0.0423\n",
      "Epoch 2640 Loss 0.0422\n",
      "Epoch 2660 Loss 0.0422\n",
      "Epoch 2680 Loss 0.0421\n",
      "Epoch 2700 Loss 0.0421\n",
      "Epoch 2720 Loss 0.0420\n",
      "Epoch 2740 Loss 0.0420\n",
      "Epoch 2760 Loss 0.0419\n",
      "Epoch 2780 Loss 0.0419\n",
      "Epoch 2800 Loss 0.0418\n",
      "Epoch 2820 Loss 0.0418\n",
      "Epoch 2840 Loss 0.0418\n",
      "Epoch 2860 Loss 0.0417\n",
      "Epoch 2880 Loss 0.0417\n",
      "Epoch 2900 Loss 0.0416\n",
      "Epoch 2920 Loss 0.0416\n",
      "Epoch 2940 Loss 0.0416\n",
      "Epoch 2960 Loss 0.0415\n",
      "Epoch 2980 Loss 0.0415\n",
      "Epoch 3000 Loss 0.0415\n",
      "Epoch 3020 Loss 0.0414\n",
      "Epoch 3040 Loss 0.0414\n",
      "Epoch 3060 Loss 0.0414\n",
      "Epoch 3080 Loss 0.0413\n",
      "Epoch 3100 Loss 0.0413\n",
      "Epoch 3120 Loss 0.0413\n",
      "Epoch 3140 Loss 0.0413\n",
      "Epoch 3160 Loss 0.0412\n",
      "Epoch 3180 Loss 0.0412\n",
      "Epoch 3200 Loss 0.0412\n",
      "Epoch 3220 Loss 0.0412\n",
      "Epoch 3240 Loss 0.0411\n",
      "Epoch 3260 Loss 0.0411\n",
      "Epoch 3280 Loss 0.0411\n",
      "Epoch 3300 Loss 0.0411\n",
      "Epoch 3320 Loss 0.0411\n",
      "Epoch 3340 Loss 0.0410\n",
      "Epoch 3360 Loss 0.0410\n",
      "Epoch 3380 Loss 0.0410\n",
      "Epoch 3400 Loss 0.0410\n",
      "Epoch 3420 Loss 0.0410\n",
      "Epoch 3440 Loss 0.0409\n",
      "Epoch 3460 Loss 0.0409\n",
      "Epoch 3480 Loss 0.0409\n",
      "Epoch 3500 Loss 0.0409\n",
      "Epoch 3520 Loss 0.0409\n",
      "Epoch 3540 Loss 0.0409\n",
      "Epoch 3560 Loss 0.0408\n",
      "Epoch 3580 Loss 0.0408\n",
      "Epoch 3600 Loss 0.0408\n",
      "Epoch 3620 Loss 0.0408\n",
      "Epoch 3640 Loss 0.0408\n",
      "Epoch 3660 Loss 0.0407\n",
      "Epoch 3680 Loss 0.0407\n",
      "Epoch 3700 Loss 0.0407\n",
      "Epoch 3720 Loss 0.0407\n",
      "Epoch 3740 Loss 0.0407\n",
      "Epoch 3760 Loss 0.0406\n",
      "Epoch 3780 Loss 0.0406\n",
      "Epoch 3800 Loss 0.0406\n",
      "Epoch 3820 Loss 0.0406\n",
      "Epoch 3840 Loss 0.0406\n",
      "Epoch 3860 Loss 0.0406\n",
      "Epoch 3880 Loss 0.0405\n",
      "Epoch 3900 Loss 0.0405\n",
      "Epoch 3920 Loss 0.0405\n",
      "Epoch 3940 Loss 0.0405\n",
      "Epoch 3960 Loss 0.0405\n",
      "Epoch 3980 Loss 0.0404\n"
     ]
    }
   ],
   "source": [
    "losses = train(X_train,Y_train,model,0.002,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c0c842ca20>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250,)\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(X_train)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9702222222222222\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(output==Y_train)/Y_train.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(pred,columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.to_csv('predict.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
