{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Logistic_X_train.csv',header='infer')\n",
    "Y_train = pd.read_csv('Logistic_Y_train.csv',header='infer')\n",
    "X_test = pd.read_csv('Logistic_X_test.csv',header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250, 2) (2250,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.values\n",
    "Y_train = Y_train.values.reshape((-1,))\n",
    "X_test = X_test.values\n",
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    e_pa = np.exp(a)\n",
    "    ans = e_pa/np.sum(e_pa,axis=1,keepdims=True)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,layers,output_size):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        \n",
    "        model= {}\n",
    "        \n",
    "        model['W1'] = np.random.randn(input_size,layers[0])\n",
    "        model['b1'] = np.zeros((1,layers[0]))\n",
    "        \n",
    "        model['W2'] = np.random.randn(layers[0],layers[1])\n",
    "        model['b2'] = np.zeros((1,layers[1]))\n",
    "        \n",
    "        model['W3'] = np.random.randn(layers[1],output_size)\n",
    "        model['b3'] = np.zeros((1,output_size))\n",
    "        \n",
    "        self.model = model\n",
    "        self.activation_output = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        b1,b2,b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        \n",
    "        z1 = np.dot(x,W1)+b1\n",
    "        a1 = np.tanh(z1)\n",
    "        \n",
    "        z2 = np.dot(a1,W2)+b2\n",
    "        a2 = np.tanh(z2)\n",
    "        \n",
    "        z3 = np.dot(a2,W3)+b3\n",
    "        y_ = softmax(z3)\n",
    "        \n",
    "        self.activation_output = (a1,a2,y_)\n",
    "        return y_\n",
    "    def backward(self,x,y,learning_rate):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        b1,b2,b3 = self.model['b1'],self.model['b2'],self.model['b3']\n",
    "        a1,a2,y_ = self.activation_output\n",
    "        \n",
    "        delta3 = y_-y\n",
    "        dw3 = np.dot(a2.T,delta3)\n",
    "        db3 = np.sum(delta3,axis=0)\n",
    "        \n",
    "        delta2 = (1-np.square(a2))*np.dot(delta3,W3.T)\n",
    "        dw2 = np.dot(a1.T,delta2)\n",
    "        db2 = np.sum(delta2,axis=0)\n",
    "        \n",
    "        delta1 = (1-np.square(a1))*np.dot(delta2,W2.T)\n",
    "        dw1 = np.dot(x.T,delta1)\n",
    "        db1 = np.sum(delta1,axis=0)\n",
    "        \n",
    "        self.model['W1'] -= learning_rate*dw1\n",
    "        self.model['b1'] -= learning_rate*db1\n",
    "        \n",
    "        self.model['W2'] -= learning_rate*dw2\n",
    "        self.model['b2'] -= learning_rate*db2\n",
    "        \n",
    "        self.model['W3'] -= learning_rate*dw3\n",
    "        self.model['b3'] -= learning_rate*db3\n",
    "        \n",
    "    def predict(self,x):\n",
    "        y_ = self.forward(x)\n",
    "        return np.argmax(y_,axis=1)\n",
    "    \n",
    "    def summary(self):\n",
    "        W1,W2,W3 = self.model['W1'],self.model['W2'],self.model['W3']\n",
    "        a1,a2,y_ = self.activation_outputs\n",
    "        \n",
    "        print(\"W1\",W1.shape)\n",
    "        print(\"A1\",a1.shape)\n",
    "        \n",
    "        print(\"W2\",W2.shape)\n",
    "        print(\"A2\",a2.shape)\n",
    "        \n",
    "        print(\"W3\",W3.shape)\n",
    "        print(\"Y_\",y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hot,p):\n",
    "    return -np.mean(y_hot*np.log(p))\n",
    "\n",
    "def one_hot(y,depth):\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    y_hot = np.zeros((m,depth))\n",
    "    y_hot[np.arange(m),y] = 1\n",
    "    \n",
    "    return y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(input_size=X_train.shape[1],layers=[10,5],output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x,y,model,learning_rate=0.001,maxItr=500,logs=True):\n",
    "    \n",
    "    training_loss = []\n",
    "    \n",
    "    classes=2\n",
    "    y_hot = one_hot(y,classes)\n",
    "    \n",
    "    for i in range(maxItr):\n",
    "        \n",
    "        Y_ = model.forward(x)\n",
    "        l = loss(y_hot,Y_)\n",
    "        training_loss.append(l)\n",
    "        model.backward(x,y_hot,learning_rate)\n",
    "        \n",
    "        if(logs and i%20==0):\n",
    "            print(\"Epoch %d Loss %.4f\"%(i,l))\n",
    "        \n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.3980\n",
      "Epoch 20 Loss 0.1540\n",
      "Epoch 40 Loss 0.1075\n",
      "Epoch 60 Loss 0.0617\n",
      "Epoch 80 Loss 0.0666\n",
      "Epoch 100 Loss 0.0980\n",
      "Epoch 120 Loss 0.0499\n",
      "Epoch 140 Loss 0.0550\n",
      "Epoch 160 Loss 0.0525\n",
      "Epoch 180 Loss 0.0514\n",
      "Epoch 200 Loss 0.0520\n",
      "Epoch 220 Loss 0.0507\n",
      "Epoch 240 Loss 0.0490\n",
      "Epoch 260 Loss 0.0513\n",
      "Epoch 280 Loss 0.0467\n",
      "Epoch 300 Loss 0.0475\n",
      "Epoch 320 Loss 0.0549\n",
      "Epoch 340 Loss 0.0444\n",
      "Epoch 360 Loss 0.0447\n",
      "Epoch 380 Loss 0.0496\n",
      "Epoch 400 Loss 0.0472\n",
      "Epoch 420 Loss 0.0446\n",
      "Epoch 440 Loss 0.0469\n",
      "Epoch 460 Loss 0.0466\n",
      "Epoch 480 Loss 0.0458\n",
      "Epoch 500 Loss 0.0463\n",
      "Epoch 520 Loss 0.0460\n",
      "Epoch 540 Loss 0.0457\n",
      "Epoch 560 Loss 0.0457\n",
      "Epoch 580 Loss 0.0456\n",
      "Epoch 600 Loss 0.0454\n",
      "Epoch 620 Loss 0.0453\n",
      "Epoch 640 Loss 0.0452\n",
      "Epoch 660 Loss 0.0451\n",
      "Epoch 680 Loss 0.0450\n",
      "Epoch 700 Loss 0.0449\n",
      "Epoch 720 Loss 0.0448\n",
      "Epoch 740 Loss 0.0446\n",
      "Epoch 760 Loss 0.0445\n",
      "Epoch 780 Loss 0.0444\n",
      "Epoch 800 Loss 0.0443\n",
      "Epoch 820 Loss 0.0442\n",
      "Epoch 840 Loss 0.0441\n",
      "Epoch 860 Loss 0.0441\n",
      "Epoch 880 Loss 0.0440\n",
      "Epoch 900 Loss 0.0439\n",
      "Epoch 920 Loss 0.0438\n",
      "Epoch 940 Loss 0.0437\n",
      "Epoch 960 Loss 0.0437\n",
      "Epoch 980 Loss 0.0436\n",
      "Epoch 1000 Loss 0.0435\n",
      "Epoch 1020 Loss 0.0435\n",
      "Epoch 1040 Loss 0.0434\n",
      "Epoch 1060 Loss 0.0434\n",
      "Epoch 1080 Loss 0.0433\n",
      "Epoch 1100 Loss 0.0433\n",
      "Epoch 1120 Loss 0.0432\n",
      "Epoch 1140 Loss 0.0432\n",
      "Epoch 1160 Loss 0.0432\n",
      "Epoch 1180 Loss 0.0431\n",
      "Epoch 1200 Loss 0.0431\n",
      "Epoch 1220 Loss 0.0430\n",
      "Epoch 1240 Loss 0.0430\n",
      "Epoch 1260 Loss 0.0430\n",
      "Epoch 1280 Loss 0.0429\n",
      "Epoch 1300 Loss 0.0429\n",
      "Epoch 1320 Loss 0.0429\n",
      "Epoch 1340 Loss 0.0428\n",
      "Epoch 1360 Loss 0.0428\n",
      "Epoch 1380 Loss 0.0428\n",
      "Epoch 1400 Loss 0.0428\n",
      "Epoch 1420 Loss 0.0427\n",
      "Epoch 1440 Loss 0.0427\n",
      "Epoch 1460 Loss 0.0427\n",
      "Epoch 1480 Loss 0.0427\n",
      "Epoch 1500 Loss 0.0426\n",
      "Epoch 1520 Loss 0.0426\n",
      "Epoch 1540 Loss 0.0426\n",
      "Epoch 1560 Loss 0.0426\n",
      "Epoch 1580 Loss 0.0425\n",
      "Epoch 1600 Loss 0.0425\n",
      "Epoch 1620 Loss 0.0425\n",
      "Epoch 1640 Loss 0.0425\n",
      "Epoch 1660 Loss 0.0424\n",
      "Epoch 1680 Loss 0.0424\n",
      "Epoch 1700 Loss 0.0424\n",
      "Epoch 1720 Loss 0.0424\n",
      "Epoch 1740 Loss 0.0424\n",
      "Epoch 1760 Loss 0.0423\n",
      "Epoch 1780 Loss 0.0423\n",
      "Epoch 1800 Loss 0.0423\n",
      "Epoch 1820 Loss 0.0423\n",
      "Epoch 1840 Loss 0.0422\n",
      "Epoch 1860 Loss 0.0422\n",
      "Epoch 1880 Loss 0.0422\n",
      "Epoch 1900 Loss 0.0422\n",
      "Epoch 1920 Loss 0.0421\n",
      "Epoch 1940 Loss 0.0421\n",
      "Epoch 1960 Loss 0.0421\n",
      "Epoch 1980 Loss 0.0421\n"
     ]
    }
   ],
   "source": [
    "losses = train(X_train,Y_train,model,0.002,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127352c5320>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250,)\n"
     ]
    }
   ],
   "source": [
    "output = model.predict(X_train)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum(output==Y_train)/Y_train.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame(pred,columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
